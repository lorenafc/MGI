{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOI76i9AfF+lQePQTp7fNr/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenafc/MGI/blob/main/ESTCScrape_chromedrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvDuEYONlBgx"
      },
      "outputs": [],
      "source": [
        "#source: chatGPT and stack overflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium --quiet\n",
        "!pip install pandas --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdBPs97ilI-Q",
        "outputId": "6c09306d-f0d9-4a55-a2c3-d033a1e19142"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.7/467.7 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade selenium chromedriver-autoinstaller --quiet"
      ],
      "metadata": {
        "id": "4VgP2jRI6DEs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get update\n",
        "# !apt-get install -y wget unzip\n",
        "# !pip install selenium\n",
        "# !wget -N https://chromedriver.storage.googleapis.com/125.0.6422.112/chromedriver_linux64.zip #original chatGPT version 114.0.5735.90\n",
        "# !unzip -o chromedriver_linux64.zip\n",
        "# !mv chromedriver /usr/local/bin/chromedriver\n"
      ],
      "metadata": {
        "id": "emCFtvy4wF1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bEOvvNIeArqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "# Set up Selenium with Chrome WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# Function to scrape a single record\n",
        "def scrape_record(record_url):\n",
        "    driver.get(record_url)\n",
        "    try:\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.XPATH, '//td[text()=\"Title\"]/following-sibling::td'))\n",
        "        )\n",
        "        alt_title = driver.find_element(By.XPATH, '//td[text()=\"Alt Title\"]/following-sibling::td').text\n",
        "        author = driver.find_element(By.XPATH, '//td[text()=\"Author\"]/following-sibling::td').text\n",
        "        city = driver.find_element(By.XPATH, '//td[text()=\"City\"]/following-sibling::td').text\n",
        "        country_code = driver.find_element(By.XPATH, '//td[text()=\"Country Code\"]/following-sibling::td').text\n",
        "        estc_no = driver.find_element(By.XPATH, '//td[text()=\"ESTC No\"]/following-sibling::td').text\n",
        "        format = driver.find_element(By.XPATH, '//td[text()=\"Format\"]/following-sibling::td').text\n",
        "        imprint_original = driver.find_element(By.XPATH, '//td[text()=\"Imprint Original\"]/following-sibling::td').text\n",
        "        language_code = driver.find_element(By.XPATH, '//td[text()=\"Language Code\"]/following-sibling::td').text\n",
        "        pagination = driver.find_element(By.XPATH, '//td[text()=\"Pagination\"]/following-sibling::td').text\n",
        "        title = driver.find_element(By.XPATH, '//td[text()=\"Title\"]/following-sibling::td').text\n",
        "        year = driver.find_element(By.XPATH, '//td[text()=\"Year\"]/following-sibling::td').text\n",
        "\n",
        "        return {\n",
        "            'Alt Title': alt_title,\n",
        "            'Author': author,\n",
        "            'City': city,\n",
        "            'Country Code': country_code,\n",
        "            'ESTC No': estc_no,\n",
        "            'Format': format,\n",
        "            'Imprint Original': imprint_original,\n",
        "            'Language Code': language_code,\n",
        "            'Pagination': pagination,\n",
        "            'Title': title,\n",
        "            'Year': year\n",
        "        }\n",
        "    except NoSuchElementException as e:\n",
        "        print(f\"Element not found in record {record_url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping record {record_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape records for a given year\n",
        "def scrape_year(year):\n",
        "    records = []\n",
        "    search_url = f\"{base_url}search?q=&year={year}&year_end={year}\"\n",
        "    print(f\"Navigating to search URL: {search_url}\")\n",
        "    driver.get(search_url)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            WebDriverWait(driver, 20).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.record\"))\n",
        "            )\n",
        "            record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record a.view-record\")\n",
        "            record_urls = [element.get_attribute('href') for element in record_elements]\n",
        "\n",
        "            print(f\"Found {len(record_urls)} records on the page.\")\n",
        "\n",
        "            for record_url in record_urls:\n",
        "                print(f\"Scraping record: {record_url}\")\n",
        "                record_data = scrape_record(record_url)\n",
        "                if record_data:\n",
        "                    records.append(record_data)\n",
        "\n",
        "            # Check if there is a next page\n",
        "            try:\n",
        "                next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "                if 'disabled' in next_button.get_attribute('class'):\n",
        "                    print(\"Next button is disabled. No more pages.\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Clicking next button to go to the next page.\")\n",
        "                    next_button.click()\n",
        "                    time.sleep(2)\n",
        "            except NoSuchElementException:\n",
        "                print(\"Next button not found. Assuming no more pages.\")\n",
        "                break\n",
        "        except TimeoutException:\n",
        "            print(\"Timed out waiting for records to load. Moving to the next page.\")\n",
        "            break\n",
        "\n",
        "    return records\n",
        "\n",
        "### TEST 1473-1475 #####\n",
        "# Scrape records year by year\n",
        "all_records = []\n",
        "for year in range(1473, 1475):\n",
        "    print(f\"Scraping year: {year}\")\n",
        "    records = scrape_year(year)\n",
        "    all_records.extend(records)\n",
        "\n",
        "# Save to CSV and Excel\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv('ESTC_records1473_1474newfunctionscrape.csv', index=False)\n",
        "df.to_excel('ESTC_records1473_1474newfunctionscrape.xlsx', index=False)\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LgABSlnDkuX",
        "outputId": "3e491261-ab8f-4946-fcef-8fc3aeae73fd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping year: 1473\n",
            "Navigating to search URL: https://estc.printprobability.org/search?q=&year=1473&year_end=1473\n",
            "Timed out waiting for records to load. Moving to the next page.\n",
            "Scraping year: 1474\n",
            "Navigating to search URL: https://estc.printprobability.org/search?q=&year=1474&year_end=1474\n",
            "Timed out waiting for records to load. Moving to the next page.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# import pandas as pd\n",
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.common.by import By\n",
        "# from selenium.webdriver.support.ui import WebDriverWait\n",
        "# from selenium.webdriver.support import expected_conditions as EC\n",
        "# from selenium.webdriver.chrome.service import Service\n",
        "# from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# # Set up Selenium with Chrome WebDriver\n",
        "# #used StackOverflow to help adjust https://stackoverflow.com/questions/76461596/unable-to-use-selenium-webdriver-getting-two-exceptions\n",
        "# chrome_options = Options()\n",
        "# chrome_options.add_argument(\"--headless\")\n",
        "# chrome_options.add_argument(\"--no-sandbox\")\n",
        "# chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# # Create the WebDriver instance\n",
        "# driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# # Test if Chrome is working\n",
        "# try:\n",
        "#     driver.get(\"https://www.google.com/\")\n",
        "#     print(\"Chrome is working\")\n",
        "# except Exception as e:\n",
        "#     print(f\"Error starting Chrome: {e}\")\n",
        "#     driver.quit()\n",
        "#     exit()\n",
        "\n",
        "# # Base URL of the website\n",
        "# base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# # Function to scrape a single record\n",
        "# def scrape_record(record_url):\n",
        "#     driver.get(record_url)\n",
        "#     try:\n",
        "#         # Adjust CSS selectors to target elements within the div with id \"root\"\n",
        "#         title = driver.find_element(By.CSS_SELECTOR, '#root div.title').text\n",
        "#         alt_title = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Alt Title\") + td').text\n",
        "#         uniform_title = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Uniform Title\") + td').text\n",
        "#         year = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Year\") + td').text\n",
        "#         country_name = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Country Name\") + td').text\n",
        "#         date = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Date\") + td').text\n",
        "#         ustc_classification = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"USTC Classification\") + td').text\n",
        "#         publication_language = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Publication Language\") + td').text\n",
        "#         marc_id = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"MARC ID\") + td').text\n",
        "\n",
        "#         return {\n",
        "#             'Title': title,\n",
        "#             'Alt Title': alt_title,\n",
        "#             'Uniform Title': uniform_title,\n",
        "#             'Year': year,\n",
        "#             'Country Name': country_name,\n",
        "#             'Date': date,\n",
        "#             'USTC Classification': ustc_classification,\n",
        "#             'Publication Language': publication_language,\n",
        "#             'MARC ID': marc_id\n",
        "#         }\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error scraping record {record_url}: {e}\")\n",
        "#         return None\n",
        "\n",
        "\n",
        "\n",
        "# # Function to scrape records for a given year\n",
        "# def scrape_year(year):\n",
        "#     records = []\n",
        "#     search_url = f\"{base_url}?master_biblio_join_filter_clean_full_holdings%5Brange%5D%5ByearAutoExtracted%5D=%3A{year}\"\n",
        "#     driver.get(search_url)\n",
        "\n",
        "#     while True:\n",
        "#         WebDriverWait(driver, 30).until(\n",
        "#             EC.presence_of_element_located((By.CSS_SELECTOR, \"div.record\"))\n",
        "#         )\n",
        "#         record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record a.view-record\")\n",
        "#         record_urls = [element.get_attribute('href') for element in record_elements]\n",
        "\n",
        "#         for record_url in record_urls:\n",
        "#             record_data = scrape_record(record_url)\n",
        "#             if record_data:\n",
        "#                 records.append(record_data)\n",
        "\n",
        "#         # Check if there is a next page\n",
        "#         try:\n",
        "#             next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "#             if 'disabled' in next_button.get_attribute('class'):\n",
        "#                 break\n",
        "#             else:\n",
        "#                 next_button.click()\n",
        "#                 time.sleep(2)\n",
        "#         except:\n",
        "#             break\n",
        "\n",
        "#     return records\n",
        "\n",
        "# # Scrape records year by year\n",
        "# all_records = []\n",
        "# for year in range(1473, 1475):\n",
        "#     print(f\"Scraping year: {year}\")\n",
        "#     records = scrape_year(year)\n",
        "#     all_records.extend(records)\n",
        "\n",
        "# # Save to CSV and Excel\n",
        "# df = pd.DataFrame(all_records)\n",
        "# df.to_csv('ESTC_books.csv', index=False)\n",
        "# df.to_excel('ESTC_books.xlsx', index=False)\n",
        "\n",
        "# # Close the driver\n",
        "# driver.quit()\n",
        "\n",
        "# print(\"Scraping completed and data saved to ESTC_books.csv and ESTC_books.xlsx.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "i-k0WGJ-Con-",
        "outputId": "7105ac25-01a8-4bc9-c495-416be212a832"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chrome is working\n",
            "Scraping year: 1473\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TimeoutException",
          "evalue": "Message: \nStacktrace:\n#0 0x5654a16a4e3a <unknown>\n#1 0x5654a138e45c <unknown>\n#2 0x5654a13da5b5 <unknown>\n#3 0x5654a13da671 <unknown>\n#4 0x5654a141ef14 <unknown>\n#5 0x5654a13fd4dd <unknown>\n#6 0x5654a141c2cc <unknown>\n#7 0x5654a13fd253 <unknown>\n#8 0x5654a13cd1c7 <unknown>\n#9 0x5654a13cdb3e <unknown>\n#10 0x5654a166b27b <unknown>\n#11 0x5654a166f327 <unknown>\n#12 0x5654a1657dae <unknown>\n#13 0x5654a166fdf2 <unknown>\n#14 0x5654a163c74f <unknown>\n#15 0x5654a1694128 <unknown>\n#16 0x5654a16942fb <unknown>\n#17 0x5654a16a3f6c <unknown>\n#18 0x7b34fb970ac3 <unknown>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6d8760ba0bc2>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1473\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1475\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Scraping year: {year}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m     \u001b[0mall_records\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6d8760ba0bc2>\u001b[0m in \u001b[0;36mscrape_year\u001b[0;34m(year)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         WebDriverWait(driver, 30).until(\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpresence_of_element_located\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"div.record\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/support/wait.py\u001b[0m in \u001b[0;36muntil\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0muntil_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n#0 0x5654a16a4e3a <unknown>\n#1 0x5654a138e45c <unknown>\n#2 0x5654a13da5b5 <unknown>\n#3 0x5654a13da671 <unknown>\n#4 0x5654a141ef14 <unknown>\n#5 0x5654a13fd4dd <unknown>\n#6 0x5654a141c2cc <unknown>\n#7 0x5654a13fd253 <unknown>\n#8 0x5654a13cd1c7 <unknown>\n#9 0x5654a13cdb3e <unknown>\n#10 0x5654a166b27b <unknown>\n#11 0x5654a166f327 <unknown>\n#12 0x5654a1657dae <unknown>\n#13 0x5654a166fdf2 <unknown>\n#14 0x5654a163c74f <unknown>\n#15 0x5654a1694128 <unknown>\n#16 0x5654a16942fb <unknown>\n#17 0x5654a16a3f6c <unknown>\n#18 0x7b34fb970ac3 <unknown>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "# Set up Selenium with Chrome WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# Function to scrape a single record\n",
        "def scrape_record(record_url):\n",
        "    driver.get(record_url)\n",
        "    try:\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.XPATH, '//td[text()=\"Title\"]/following-sibling::td'))\n",
        "        )\n",
        "        alt_title = driver.find_element(By.XPATH, '//td[text()=\"Alt Title\"]/following-sibling::td').text\n",
        "        author = driver.find_element(By.XPATH, '//td[text()=\"Author\"]/following-sibling::td').text\n",
        "        city = driver.find_element(By.XPATH, '//td[text()=\"City\"]/following-sibling::td').text\n",
        "        country_code = driver.find_element(By.XPATH, '//td[text()=\"Country Code\"]/following-sibling::td').text\n",
        "        estc_no = driver.find_element(By.XPATH, '//td[text()=\"ESTC No\"]/following-sibling::td').text\n",
        "        format = driver.find_element(By.XPATH, '//td[text()=\"Format\"]/following-sibling::td').text\n",
        "        imprint_original = driver.find_element(By.XPATH, '//td[text()=\"Imprint Original\"]/following-sibling::td').text\n",
        "        language_code = driver.find_element(By.XPATH, '//td[text()=\"Language Code\"]/following-sibling::td').text\n",
        "        pagination = driver.find_element(By.XPATH, '//td[text()=\"Pagination\"]/following-sibling::td').text\n",
        "        title = driver.find_element(By.XPATH, '//td[text()=\"Title\"]/following-sibling::td').text\n",
        "        year = driver.find_element(By.XPATH, '//td[text()=\"Year\"]/following-sibling::td').text\n",
        "\n",
        "        return {\n",
        "            'Alt Title': alt_title,\n",
        "            'Author': author,\n",
        "            'City': city,\n",
        "            'Country Code': country_code,\n",
        "            'ESTC No': estc_no,\n",
        "            'Format': format,\n",
        "            'Imprint Original': imprint_original,\n",
        "            'Language Code': language_code,\n",
        "            'Pagination': pagination,\n",
        "            'Title': title,\n",
        "            'Year': year\n",
        "        }\n",
        "    except NoSuchElementException as e:\n",
        "        print(f\"Element not found in record {record_url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping record {record_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape records for a given year\n",
        "def scrape_year(year):\n",
        "    records = []\n",
        "    driver.get(base_url)\n",
        "\n",
        "    try:\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.CLASS_NAME, \"ais-Dropdown-button\"))\n",
        "        )\n",
        "\n",
        "        # Click the \"Year\" button to open the dropdown\n",
        "        year_button = driver.find_element(By.CLASS_NAME, \"ais-Dropdown-button\")\n",
        "        year_button.click()\n",
        "\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.CLASS_NAME, \"ais-RangeInput-input--min\"))\n",
        "        )\n",
        "\n",
        "        # Input the start year\n",
        "        start_year_input = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-input--min\")\n",
        "        start_year_input.clear()\n",
        "        start_year_input.send_keys(str(year))\n",
        "\n",
        "        # Input the end year\n",
        "        end_year_input = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-input--max\")\n",
        "        end_year_input.clear()\n",
        "        end_year_input.send_keys(str(year))\n",
        "\n",
        "        # Click the \"Go\" button\n",
        "        go_button = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-submit\")\n",
        "        go_button.click()\n",
        "\n",
        "        while True:\n",
        "            WebDriverWait(driver, 40).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.record\"))\n",
        "            )\n",
        "            record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record a.view-record\")\n",
        "\n",
        "            if not record_elements:\n",
        "                print(f\"No record elements found on the page for year {year}.\")\n",
        "                break\n",
        "\n",
        "            record_urls = [element.get_attribute('href') for element in record_elements]\n",
        "            print(f\"Found {len(record_urls)} records on the page.\")\n",
        "\n",
        "            for record_url in record_urls:\n",
        "                print(f\"Scraping record: {record_url}\")\n",
        "                record_data = scrape_record(record_url)\n",
        "                if record_data:\n",
        "                    records.append(record_data)\n",
        "\n",
        "            # Check if there is a next page\n",
        "            try:\n",
        "                next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "                if 'disabled' in next_button.get_attribute('class'):\n",
        "                    print(\"Next button is disabled. No more pages.\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Clicking next button to go to the next page.\")\n",
        "                    next_button.click()\n",
        "                    time.sleep(2)\n",
        "            except NoSuchElementException:\n",
        "                print(\"Next button not found. Assuming no more pages.\")\n",
        "                break\n",
        "    except TimeoutException:\n",
        "        print(\"Timed out waiting for records to load. Moving to the next page.\")\n",
        "\n",
        "    return records\n",
        "\n",
        "### TEST 1473-1475 #####\n",
        "# Scrape records year by year\n",
        "all_records = []\n",
        "for year in range(1473, 1475):\n",
        "    print(f\"Scraping year: {year}\")\n",
        "    records = scrape_year(year)\n",
        "    all_records.extend(records)\n",
        "\n",
        "# Save to CSV and Excel\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv('ESTC_records1473_1474newfunctionscrape.csv', index=False)\n",
        "df.to_excel('ESTC_records1473_1474newfunctionscrape.xlsx', index=False)\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2DUzILZHc7l",
        "outputId": "6c099748-4879-4d1b-dc31-230dbd81d99e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping year: 1473\n",
            "Timed out waiting for records to load. Moving to the next page.\n",
            "Scraping year: 1474\n",
            "Timed out waiting for records to load. Moving to the next page.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.common.by import By\n",
        "# from selenium.webdriver.support.ui import WebDriverWait\n",
        "# from selenium.webdriver.support import expected_conditions as EC\n",
        "# from selenium.webdriver.chrome.service import Service\n",
        "# from selenium.webdriver.chrome.options import Options\n",
        "# import pandas as pd\n",
        "# import time\n",
        "# from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "# # Set up Selenium with Chrome WebDriver\n",
        "# chrome_options = Options()\n",
        "# chrome_options.add_argument(\"--headless\")\n",
        "# chrome_options.add_argument(\"--no-sandbox\")\n",
        "# chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# # Initialize the Chrome WebDriver\n",
        "# driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# # Base URL of the website\n",
        "# base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# # Function to scrape a single record\n",
        "# def scrape_record(record_url):\n",
        "#     driver.get(record_url)\n",
        "#     try:\n",
        "#         WebDriverWait(driver, 20).until(\n",
        "#             EC.presence_of_element_located((By.XPATH, '//td[text()=\"Title\"]/following-sibling::td'))\n",
        "#         )\n",
        "#         alt_title = driver.find_element(By.XPATH, '//td[text()=\"Alt Title\"]/following-sibling::td').text\n",
        "#         author = driver.find_element(By.XPATH, '//td[text()=\"Author\"]/following-sibling::td').text\n",
        "#         city = driver.find_element(By.XPATH, '//td[text()=\"City\"]/following-sibling::td').text\n",
        "#         country_code = driver.find_element(By.XPATH, '//td[text()=\"Country Code\"]/following-sibling::td').text\n",
        "#         estc_no = driver.find_element(By.XPATH, '//td[text()=\"ESTC No\"]/following-sibling::td').text\n",
        "#         format = driver.find_element(By.XPATH, '//td[text()=\"Format\"]/following-sibling::td').text\n",
        "#         imprint_original = driver.find_element(By.XPATH, '//td[text()=\"Imprint Original\"]/following-sibling::td').text\n",
        "#         language_code = driver.find_element(By.XPATH, '//td[text()=\"Language Code\"]/following-sibling::td').text\n",
        "#         pagination = driver.find_element(By.XPATH, '//td[text()=\"Pagination\"]/following-sibling::td').text\n",
        "#         title = driver.find_element(By.XPATH, '//td[text()=\"Title\"]/following-sibling::td').text\n",
        "#         year = driver.find_element(By.XPATH, '//td[text()=\"Year\"]/following-sibling::td').text\n",
        "\n",
        "#         return {\n",
        "#             'Alt Title': alt_title,\n",
        "#             'Author': author,\n",
        "#             'City': city,\n",
        "#             'Country Code': country_code,\n",
        "#             'ESTC No': estc_no,\n",
        "#             'Format': format,\n",
        "#             'Imprint Original': imprint_original,\n",
        "#             'Language Code': language_code,\n",
        "#             'Pagination': pagination,\n",
        "#             'Title': title,\n",
        "#             'Year': year\n",
        "#         }\n",
        "#     except NoSuchElementException as e:\n",
        "#         print(f\"Element not found in record {record_url}: {e}\")\n",
        "#         return None\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error scraping record {record_url}: {e}\")\n",
        "#         return None\n",
        "\n",
        "# # Function to scrape records for a given year\n",
        "# def scrape_year(year):\n",
        "#     records = []\n",
        "#     driver.get(base_url)\n",
        "\n",
        "#     print(f\"Navigating to search URL: {base_url} Year search: {year}\")\n",
        "\n",
        "#     try:\n",
        "#         WebDriverWait(driver, 60).until(\n",
        "#             EC.presence_of_element_located((By.CLASS_NAME, \"ais-Dropdown-button\"))\n",
        "#         )\n",
        "\n",
        "#         # Click the \"Year\" button to open the dropdown\n",
        "#         print(\"Clicking the 'Year' button\")\n",
        "#         year_button = driver.find_element(By.CLASS_NAME, \"ais-Dropdown-button\")\n",
        "#         year_button.click()\n",
        "\n",
        "#         WebDriverWait(driver, 60).until(\n",
        "#             EC.presence_of_element_located((By.CLASS_NAME, \"ais-RangeInput-input--min\"))\n",
        "#         )\n",
        "\n",
        "#         # Input the start year\n",
        "#         print(f\"Inputting the start year: {year}\")\n",
        "#         start_year_input = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-input--min\")\n",
        "#         start_year_input.clear()\n",
        "#         start_year_input.send_keys(str(year))\n",
        "\n",
        "#         # Input the end year\n",
        "#         print(f\"Inputting the end year: {year}\")\n",
        "#         end_year_input = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-input--max\")\n",
        "#         end_year_input.clear()\n",
        "#         end_year_input.send_keys(str(year))\n",
        "\n",
        "#         # Click the \"Go\" button\n",
        "#         print(\"Clicking the 'Go' button\")\n",
        "#         go_button = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-submit\")\n",
        "#         go_button.click()\n",
        "\n",
        "#         while True:\n",
        "#             WebDriverWait(driver, 60).until(\n",
        "#                 EC.presence_of_element_located((By.CSS_SELECTOR, \"div.record\"))\n",
        "#             )\n",
        "#             record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record a.view-record\")\n",
        "\n",
        "#             if not record_elements:\n",
        "#                 print(f\"No record elements found on the page for year {year}.\")\n",
        "#                 break\n",
        "\n",
        "#             record_urls = [element.get_attribute('href') for element in record_elements]\n",
        "#             print(f\"Found {len(record_urls)} records on the page.\")\n",
        "\n",
        "#             for record_url in record_urls:\n",
        "#                 print(f\"Scraping record: {record_url}\")\n",
        "#                 record_data = scrape_record(record_url)\n",
        "#                 if record_data:\n",
        "#                     records.append(record_data)\n",
        "\n",
        "#             # Check if there is a next page\n",
        "#             try:\n",
        "#                 next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "#                 if 'disabled' in next_button.get_attribute('class'):\n",
        "#                     print(\"Next button is disabled. No more pages.\")\n",
        "#                     break\n",
        "#                 else:\n",
        "#                     print(\"Clicking next button to go to the next page.\")\n",
        "#                     next_button.click()\n",
        "#                     time.sleep(2)\n",
        "#             except NoSuchElementException:\n",
        "#                 print(\"Next button not found. Assuming no more pages.\")\n",
        "#                 break\n",
        "#     except TimeoutException:\n",
        "#         print(\"Timed out waiting for records to load. Moving to the next page.\")\n",
        "\n",
        "#     return records\n",
        "\n",
        "# ########\n",
        "\n",
        "\n",
        "# ### TEST 1473-1475 #####\n",
        "# # Scrape records year by year\n",
        "# all_records = []\n",
        "# for year in range(1473, 1475):\n",
        "#     print(f\"Scraping year: {year}\")\n",
        "#     records = scrape_year(year)\n",
        "#     all_records.extend(records)\n",
        "\n",
        "# # Save to CSV and Excel\n",
        "# df = pd.DataFrame(all_records)\n",
        "# df.to_csv('ESTC_records1473_1474newfunctionscrape.csv', index=False)\n",
        "# df.to_excel('ESTC_records1473_1474newfunctionscrape.xlsx', index=False)\n",
        "\n",
        "# # Close the driver\n",
        "# driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY4DYsLeJjqZ",
        "outputId": "9c824e74-f308-44d5-d4a3-23b31029ea89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping year: 1473\n",
            "Navigating to search URL: https://estc.printprobability.org/ Year search: 1473\n",
            "Clicking the 'Year' button\n",
            "Inputting the start year: 1473\n",
            "Inputting the end year: 1473\n",
            "Clicking the 'Go' button\n",
            "Timed out waiting for records to load. Moving to the next page.\n",
            "Scraping year: 1474\n",
            "Navigating to search URL: https://estc.printprobability.org/ Year search: 1474\n",
            "Clicking the 'Year' button\n",
            "Inputting the start year: 1474\n",
            "Inputting the end year: 1474\n",
            "Clicking the 'Go' button\n",
            "Timed out waiting for records to load. Moving to the next page.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.common.by import By\n",
        "# from selenium.webdriver.support.ui import WebDriverWait\n",
        "# from selenium.webdriver.support import expected_conditions as EC\n",
        "# from selenium.webdriver.chrome.service import Service\n",
        "# from selenium.webdriver.chrome.options import Options\n",
        "# import pandas as pd\n",
        "# import time\n",
        "# from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "# # Set up Selenium with Chrome WebDriver\n",
        "# chrome_options = Options()\n",
        "# chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
        "# chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
        "# chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
        "\n",
        "# # Initialize the Chrome WebDriver\n",
        "# driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# # Base URL of the website\n",
        "# base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# # Function to scrape a single record\n",
        "# def scrape_record(record_url):\n",
        "#     \"\"\"\n",
        "#     Scrape data from a single record URL.\n",
        "#     \"\"\"\n",
        "#     driver.get(record_url)\n",
        "#     try:\n",
        "#         WebDriverWait(driver, 20).until(\n",
        "#             EC.presence_of_element_located((By.XPATH, '//td[text()=\"Title\"]/following-sibling::td'))\n",
        "#         )\n",
        "#         # Extract data from the record page\n",
        "#         alt_title = driver.find_element(By.XPATH, '//td[text()=\"Alt Title\"]/following-sibling::td').text\n",
        "#         author = driver.find_element(By.XPATH, '//td[text()=\"Author\"]/following-sibling::td').text\n",
        "#         city = driver.find_element(By.XPATH, '//td[text()=\"City\"]/following-sibling::td').text\n",
        "#         country_code = driver.find_element(By.XPATH, '//td[text()=\"Country Code\"]/following-sibling::td').text\n",
        "#         estc_no = driver.find_element(By.XPATH, '//td[text()=\"ESTC No\"]/following-sibling::td').text\n",
        "#         format = driver.find_element(By.XPATH, '//td[text()=\"Format\"]/following-sibling::td').text\n",
        "#         imprint_original = driver.find_element(By.XPATH, '//td[text()=\"Imprint Original\"]/following-sibling::td').text\n",
        "#         language_code = driver.find_element(By.XPATH, '//td[text()=\"Language Code\"]/following-sibling::td').text\n",
        "#         pagination = driver.find_element(By.XPATH, '//td[text()=\"Pagination\"]/following-sibling::td').text\n",
        "#         title = driver.find_element(By.XPATH, '//td[text()=\"Title\"]/following-sibling::td').text\n",
        "#         year = driver.find_element(By.XPATH, '//td[text()=\"Year\"]/following-sibling::td').text\n",
        "\n",
        "#         return {\n",
        "#             'Alt Title': alt_title,\n",
        "#             'Author': author,\n",
        "#             'City': city,\n",
        "#             'Country Code': country_code,\n",
        "#             'ESTC No': estc_no,\n",
        "#             'Format': format,\n",
        "#             'Imprint Original': imprint_original,\n",
        "#             'Language Code': language_code,\n",
        "#             'Pagination': pagination,\n",
        "#             'Title': title,\n",
        "#             'Year': year\n",
        "#         }\n",
        "#     except NoSuchElementException as e:\n",
        "#         print(f\"Element not found in record {record_url}: {e}\")\n",
        "#         return None\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error scraping record {record_url}: {e}\")\n",
        "#         return None\n",
        "\n",
        "# # Function to scrape records for a given year\n",
        "# def scrape_year(year):\n",
        "#     \"\"\"\n",
        "#     Scrape records for a given year.\n",
        "#     \"\"\"\n",
        "#     records = []\n",
        "#     driver.get(base_url)\n",
        "\n",
        "#     print(f\"Navigating to search URL: {base_url} Year search: {year}\")\n",
        "\n",
        "#     try:\n",
        "#         WebDriverWait(driver, 60).until(\n",
        "#             EC.presence_of_element_located((By.CLASS_NAME, \"ais-Dropdown-button\"))\n",
        "#         )\n",
        "\n",
        "#         # Click the \"Year\" button to open the dropdown\n",
        "#         print(\"Clicking the 'Year' button\")\n",
        "#         year_button = driver.find_element(By.CLASS_NAME, \"ais-Dropdown-button\")\n",
        "#         year_button.click()\n",
        "\n",
        "#         WebDriverWait(driver, 60).until(\n",
        "#             EC.presence_of_element_located((By.CLASS_NAME, \"ais-RangeInput-input--min\"))\n",
        "#         )\n",
        "\n",
        "#         # Input the start year\n",
        "#         print(f\"Inputting the start year: {year}\")\n",
        "#         start_year_input = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-input--min\")\n",
        "#         start_year_input.clear()\n",
        "#         start_year_input.send_keys(str(year))\n",
        "\n",
        "#         # Input the end year\n",
        "#         print(f\"Inputting the end year: {year}\")\n",
        "#         end_year_input = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-input--max\")\n",
        "#         end_year_input.clear()\n",
        "#         end_year_input.send_keys(str(year))\n",
        "\n",
        "#         # Click the \"Go\" button\n",
        "#         print(\"Clicking the 'Go' button\")\n",
        "#         go_button = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-submit\")\n",
        "#         go_button.click()\n",
        "\n",
        "#         while True:\n",
        "#             WebDriverWait(driver, 60).until(\n",
        "#                 EC.presence_of_element_located((By.CSS_SELECTOR, \"div.record\"))\n",
        "#             )\n",
        "#             record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record a.view-record\")\n",
        "\n",
        "#             if not record_elements:\n",
        "#                 print(f\"No record elements found on the page for year {year}.\")\n",
        "#                 break\n",
        "\n",
        "#             record_urls = [element.get_attribute('href') for element in record_elements]\n",
        "#             print(f\"Found {len(record_urls)} records on the page.\")\n",
        "\n",
        "#             for record_url in record_urls:\n",
        "#                 print(f\"Scraping record: {record_url}\")\n",
        "#                 record_data = scrape_record(record_url)\n",
        "#                 if record_data:\n",
        "#                     records.append(record_data)\n",
        "\n",
        "#             # Check if there is a next page\n",
        "#             try:\n",
        "#                 next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "#                 if 'disabled' in next_button.get_attribute('class'):\n",
        "#                     print(\"Next button is disabled. No more pages.\")\n",
        "#                     break\n",
        "#                 else:\n",
        "#                     print(\"Clicking next button to go to the next page.\")\n",
        "#                     next_button.click()\n",
        "#                     time.sleep(2)\n",
        "#             except NoSuchElementException:\n",
        "#                 print(\"Next button not found. Assuming no more pages.\")\n",
        "#                 break\n",
        "#     except TimeoutException:\n",
        "#         print(\"Timed out waiting for records to load. Moving to the next page.\")\n",
        "\n",
        "#     return records\n",
        "\n",
        "# ### TEST 1473-1475 #####\n",
        "# # Scrape records year by year\n",
        "# all_records = []\n",
        "# for year in range(1473, 1475):\n",
        "#     print(f\"Scraping year: {year}\")\n",
        "#     records = scrape_year(year)\n",
        "#     all_records.extend(records)\n",
        "\n",
        "# # Save to CSV and Excel\n",
        "# df = pd.DataFrame(all_records)\n",
        "# df.to_csv('ESTC_records1473_1474newfunctionscrape.csv', index=False)\n",
        "# df.to_excel('ESTC_records1473_1474newfunctionscrape.xlsx', index=False)\n",
        "\n",
        "# # Close the driver\n",
        "# driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMLTh2CsfMyf",
        "outputId": "93455267-8135-4238-c671-6ff8fe048079"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping year: 1473\n",
            "Navigating to search URL: https://estc.printprobability.org/ Year search: 1473\n",
            "Clicking the 'Year' button\n",
            "Inputting the start year: 1473\n",
            "Inputting the end year: 1473\n",
            "Clicking the 'Go' button\n",
            "Timed out waiting for records to load. Moving to the next page.\n",
            "Scraping year: 1474\n",
            "Navigating to search URL: https://estc.printprobability.org/ Year search: 1474\n",
            "Clicking the 'Year' button\n",
            "Inputting the start year: 1474\n",
            "Inputting the end year: 1474\n",
            "Clicking the 'Go' button\n",
            "Timed out waiting for records to load. Moving to the next page.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ##### TESTING SCRAPE WITHOUT USING YEAR BUTTON SEARCH:\n",
        "# from selenium import webdriver\n",
        "# from selenium.webdriver.common.by import By\n",
        "# from selenium.webdriver.support.ui import WebDriverWait\n",
        "# from selenium.webdriver.support import expected_conditions as EC\n",
        "# from selenium.webdriver.chrome.service import Service\n",
        "# from selenium.webdriver.chrome.options import Options\n",
        "# import pandas as pd\n",
        "# import time\n",
        "# from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "# # Set up Selenium with Chrome WebDriver\n",
        "# chrome_options = Options()\n",
        "# chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
        "# chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
        "# chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
        "\n",
        "# # Initialize the Chrome WebDriver\n",
        "# driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# # Base URL of the website\n",
        "# base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# # Function to scrape a single record\n",
        "# def scrape_record(record_element):\n",
        "#     \"\"\"\n",
        "#     Scrape data from a single record element.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         # Extract data from the record element\n",
        "#         author = record_element.find_element(By.CSS_SELECTOR, 'div.authors').text.strip()\n",
        "#         estc_no = record_element.find_element(By.CSS_SELECTOR, 'div.estcNo').text.strip()\n",
        "#         year = record_element.find_element(By.CSS_SELECTOR, 'div.year').text.strip()\n",
        "\n",
        "#         return {\n",
        "#             'Author': author,\n",
        "#             'ESTC No': estc_no,\n",
        "#             'Year': year\n",
        "#         }\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error scraping record: {e}\")\n",
        "#         return None\n",
        "\n",
        "# # Function to scrape records from the current page\n",
        "# # def scrape_current_page():\n",
        "# #     \"\"\"\n",
        "# #     Scrape records from the current page.\n",
        "# #     \"\"\"\n",
        "# #     records = []\n",
        "# #     try:\n",
        "# #         record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record\")\n",
        "# #         print(f\"Found {len(record_elements)} records on the page.\")\n",
        "\n",
        "# #         for record_element in record_elements:\n",
        "# #             record_data = scrape_record(record_element)\n",
        "# #             if record_data:\n",
        "# #                 records.append(record_data)\n",
        "\n",
        "# #         return records\n",
        "# #     except Exception as e:\n",
        "# #         print(f\"Error scraping current page: {e}\")\n",
        "# #         return None\n",
        "\n",
        "# # Function to scrape records from the current page\n",
        "# def scrape_current_page():\n",
        "#     \"\"\"\n",
        "#     Scrape records from the current page.\n",
        "#     \"\"\"\n",
        "#     records = []\n",
        "#     try:\n",
        "#         record_elements = driver.find_elements(By.CSS_SELECTOR, \"div[data-key]\")\n",
        "#         print(f\"Found {len(record_elements)} records on the page.\")\n",
        "\n",
        "#         for record_element in record_elements:\n",
        "#             record_data = scrape_record(record_element)\n",
        "#             if record_data:\n",
        "#                 records.append(record_data)\n",
        "\n",
        "#         return records\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error scraping current page: {e}\")\n",
        "#         return None\n",
        "\n",
        "\n",
        "# # Function to check if there's a next page and navigate to it\n",
        "# def go_to_next_page():\n",
        "#     \"\"\"\n",
        "#     Check if there's a next page and navigate to it.\n",
        "#     \"\"\"\n",
        "#     try:\n",
        "#         next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "#         if 'disabled' not in next_button.get_attribute('class'):\n",
        "#             print(\"Clicking next button to go to the next page.\")\n",
        "#             next_button.click()\n",
        "#             time.sleep(2)\n",
        "#             return True\n",
        "#         else:\n",
        "#             print(\"Next button is disabled. No more pages.\")\n",
        "#             return False\n",
        "#     except NoSuchElementException:\n",
        "#         print(\"Next button not found. Assuming no more pages.\")\n",
        "#         return False\n",
        "\n",
        "# ### MAIN SCRIPT ###\n",
        "\n",
        "# # Navigate to the base URL\n",
        "# driver.get(base_url)\n",
        "\n",
        "# # Scrape records from the main page and navigate to the next page if available\n",
        "# all_records = []\n",
        "# while True:\n",
        "#     print(\"Scraping records from the current page.\")\n",
        "#     records = scrape_current_page()\n",
        "#     if records:\n",
        "#         all_records.extend(records)\n",
        "\n",
        "#     if not go_to_next_page():\n",
        "#         break\n",
        "\n",
        "# # Save to CSV and Excel\n",
        "# df = pd.DataFrame(all_records)\n",
        "# df.to_csv('ESTC_records_main_page.csv', index=False)\n",
        "# df.to_excel('ESTC_records_main_page.xlsx', index=False)\n",
        "\n",
        "# # Close the driver\n",
        "# driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fywcPkVfh5Cy",
        "outputId": "24093869-0ca6-4992-f91f-85c19fce83cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping records from the current page.\n",
            "Found 0 records on the page.\n",
            "Next button not found. Assuming no more pages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import requests\n",
        "# from bs4 import BeautifulSoup\n",
        "\n",
        "# # Function to scrape the required information from a single page\n",
        "# def scrape_page(url):\n",
        "#     response = requests.get(url)\n",
        "#     soup = BeautifulSoup(response.content, 'html.parser')\n",
        "#     # Extracting data from each record\n",
        "#     records = []\n",
        "#     for hit in soup.find_all('li', class_='ais-Hits-item'):\n",
        "#         record = {}\n",
        "#         record['title'] = hit.find('h3').text.strip()\n",
        "#         record['author'] = hit.find('p').find('strong').next_sibling.strip()[8:]\n",
        "#         record['estc_no'] = hit.find('p', class_='ais-Highlight').text.strip()\n",
        "#         record['year'] = hit.find('p', class_='ais-Highlight').find_next_sibling('p').text.strip()[6:]\n",
        "#         records.append(record)\n",
        "#     return records\n",
        "\n",
        "# # Function to scrape multiple pages\n",
        "# def scrape_multiple_pages(base_url, num_pages):\n",
        "#     all_records = []\n",
        "#     for page_num in range(1, num_pages + 1):\n",
        "#         url = f\"{base_url}?master_biblio_join_filter_clean_full_holdings%5Bpage%5D={page_num}\"\n",
        "#         records = scrape_page(url)\n",
        "#         all_records.extend(records)\n",
        "#     return all_records\n",
        "\n",
        "# # Main function\n",
        "# def main():\n",
        "#     base_url = \"https://estc.printprobability.org/\"\n",
        "#     num_pages = 2  # Number of pages to scrape\n",
        "#     all_records = scrape_multiple_pages(base_url, num_pages)\n",
        "#     # Printing scraped records\n",
        "#     for record in all_records:\n",
        "#         print(\"Title:\", record['title'])\n",
        "#         print(\"Author:\", record['author'])\n",
        "#         print(\"ESTC No:\", record['estc_no'])\n",
        "#         print(\"Year:\", record['year'])\n",
        "#         print()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "eox0G8PKh8m5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}