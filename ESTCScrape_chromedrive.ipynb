{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi8se/7yQcbs0qtqsZ6/NT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenafc/MGI/blob/main/ESTCScrape_chromedrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvDuEYONlBgx"
      },
      "outputs": [],
      "source": [
        "#source: chatGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium --quiet\n",
        "!pip install pandas --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdBPs97ilI-Q",
        "outputId": "b1e2a6b4-9a0d-4f3e-dfc8-4a3939e28583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.7/467.7 kB\u001b[0m \u001b[31m38.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade selenium chromedriver-autoinstaller --quiet"
      ],
      "metadata": {
        "id": "4VgP2jRI6DEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !apt-get update\n",
        "# !apt-get install -y wget unzip\n",
        "# !pip install selenium\n",
        "# !wget -N https://chromedriver.storage.googleapis.com/125.0.6422.112/chromedriver_linux64.zip #original chatGPT version 114.0.5735.90\n",
        "# !unzip -o chromedriver_linux64.zip\n",
        "# !mv chromedriver /usr/local/bin/chromedriver\n"
      ],
      "metadata": {
        "id": "emCFtvy4wF1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import pandas as pd\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "# Set up Selenium with Chrome WebDriver\n",
        "#used StackOverflow to help adjust https://stackoverflow.com/questions/76461596/unable-to-use-selenium-webdriver-getting-two-exceptions\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Create the WebDriver instance\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Test if Chrome is working\n",
        "try:\n",
        "    driver.get(\"https://www.google.com/\")\n",
        "    print(\"Chrome is working\")\n",
        "except Exception as e:\n",
        "    print(f\"Error starting Chrome: {e}\")\n",
        "    driver.quit()\n",
        "    exit()\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# Function to scrape a single record\n",
        "def scrape_record(record_url):\n",
        "    driver.get(record_url)\n",
        "    try:\n",
        "        # Adjust CSS selectors to target elements within the div with id \"root\"\n",
        "        title = driver.find_element(By.CSS_SELECTOR, '#root div.title').text\n",
        "        alt_title = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Alt Title\") + td').text\n",
        "        uniform_title = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Uniform Title\") + td').text\n",
        "        year = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Year\") + td').text\n",
        "        country_name = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Country Name\") + td').text\n",
        "        date = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Date\") + td').text\n",
        "        ustc_classification = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"USTC Classification\") + td').text\n",
        "        publication_language = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"Publication Language\") + td').text\n",
        "        marc_id = driver.find_element(By.CSS_SELECTOR, '#root td:contains(\"MARC ID\") + td').text\n",
        "\n",
        "        return {\n",
        "            'Title': title,\n",
        "            'Alt Title': alt_title,\n",
        "            'Uniform Title': uniform_title,\n",
        "            'Year': year,\n",
        "            'Country Name': country_name,\n",
        "            'Date': date,\n",
        "            'USTC Classification': ustc_classification,\n",
        "            'Publication Language': publication_language,\n",
        "            'MARC ID': marc_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping record {record_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "# Function to scrape records for a given year\n",
        "def scrape_year(year):\n",
        "    records = []\n",
        "    search_url = f\"{base_url}?master_biblio_join_filter_clean_full_holdings%5Brange%5D%5ByearAutoExtracted%5D=%3A{year}\"\n",
        "    driver.get(search_url)\n",
        "\n",
        "    while True:\n",
        "        WebDriverWait(driver, 30).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.record\"))\n",
        "        )\n",
        "        record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record a.view-record\")\n",
        "        record_urls = [element.get_attribute('href') for element in record_elements]\n",
        "\n",
        "        for record_url in record_urls:\n",
        "            record_data = scrape_record(record_url)\n",
        "            if record_data:\n",
        "                records.append(record_data)\n",
        "\n",
        "        # Check if there is a next page\n",
        "        try:\n",
        "            next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "            if 'disabled' in next_button.get_attribute('class'):\n",
        "                break\n",
        "            else:\n",
        "                next_button.click()\n",
        "                time.sleep(2)\n",
        "        except:\n",
        "            break\n",
        "\n",
        "    return records\n",
        "\n",
        "# Scrape records year by year\n",
        "all_records = []\n",
        "for year in range(1463, 1881):\n",
        "    print(f\"Scraping year: {year}\")\n",
        "    records = scrape_year(year)\n",
        "    all_records.extend(records)\n",
        "\n",
        "# Save to CSV and Excel\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv('ESTC_books.csv', index=False)\n",
        "df.to_excel('ESTC_books.xlsx', index=False)\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n",
        "\n",
        "print(\"Scraping completed and data saved to ESTC_books.csv and ESTC_books.xlsx.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "i-k0WGJ-Con-",
        "outputId": "f3a365ca-a0a5-43b3-cac0-50eed478594a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chrome is working\n",
            "Scraping year: 1463\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TimeoutException",
          "evalue": "Message: \nStacktrace:\n#0 0x56c6cf484e8a <unknown>\n#1 0x56c6cf16e45c <unknown>\n#2 0x56c6cf1ba5b5 <unknown>\n#3 0x56c6cf1ba671 <unknown>\n#4 0x56c6cf1fef14 <unknown>\n#5 0x56c6cf1dd4dd <unknown>\n#6 0x56c6cf1fc2cc <unknown>\n#7 0x56c6cf1dd253 <unknown>\n#8 0x56c6cf1ad1c7 <unknown>\n#9 0x56c6cf1adb3e <unknown>\n#10 0x56c6cf44b2cb <unknown>\n#11 0x56c6cf44f377 <unknown>\n#12 0x56c6cf437dfe <unknown>\n#13 0x56c6cf44fe42 <unknown>\n#14 0x56c6cf41c79f <unknown>\n#15 0x56c6cf474178 <unknown>\n#16 0x56c6cf47434b <unknown>\n#17 0x56c6cf483fbc <unknown>\n#18 0x7ffac2585ac3 <unknown>\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5a9e77e542c1>\u001b[0m in \u001b[0;36m<cell line: 125>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myear\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1463\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1881\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Scraping year: {year}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_year\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m     \u001b[0mall_records\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5a9e77e542c1>\u001b[0m in \u001b[0;36mscrape_year\u001b[0;34m(year)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         WebDriverWait(driver, 30).until(\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mEC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpresence_of_element_located\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"div.record\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/selenium/webdriver/support/wait.py\u001b[0m in \u001b[0;36muntil\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0muntil_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n#0 0x56c6cf484e8a <unknown>\n#1 0x56c6cf16e45c <unknown>\n#2 0x56c6cf1ba5b5 <unknown>\n#3 0x56c6cf1ba671 <unknown>\n#4 0x56c6cf1fef14 <unknown>\n#5 0x56c6cf1dd4dd <unknown>\n#6 0x56c6cf1fc2cc <unknown>\n#7 0x56c6cf1dd253 <unknown>\n#8 0x56c6cf1ad1c7 <unknown>\n#9 0x56c6cf1adb3e <unknown>\n#10 0x56c6cf44b2cb <unknown>\n#11 0x56c6cf44f377 <unknown>\n#12 0x56c6cf437dfe <unknown>\n#13 0x56c6cf44fe42 <unknown>\n#14 0x56c6cf41c79f <unknown>\n#15 0x56c6cf474178 <unknown>\n#16 0x56c6cf47434b <unknown>\n#17 0x56c6cf483fbc <unknown>\n#18 0x7ffac2585ac3 <unknown>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "\n",
        "# Set up Selenium with Chrome WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# Function to scrape a single record\n",
        "# def scrape_record(record_url):\n",
        "#     driver.get(record_url)\n",
        "#     try:\n",
        "#         title = driver.find_element(By.CSS_SELECTOR, 'div.title').text\n",
        "#         author = driver.find_element(By.CSS_SELECTOR, 'div.author').text\n",
        "#         year = driver.find_element(By.CSS_SELECTOR, 'div.year').text\n",
        "#         return {'Title': title, 'Author': author, 'Year': year}\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error scraping record {record_url}: {e}\")\n",
        "#         return None\n",
        "\n",
        "# Function to scrape a single record\n",
        "def scrape_record(record_url):\n",
        "    driver.get(record_url)\n",
        "    try:\n",
        "        alt_title = driver.find_element(By.XPATH, '//td[text()=\"Alt Title\"]/following-sibling::td').text\n",
        "        author = driver.find_element(By.XPATH, '//td[text()=\"Author\"]/following-sibling::td').text\n",
        "        city = driver.find_element(By.XPATH, '//td[text()=\"City\"]/following-sibling::td').text\n",
        "        country_code = driver.find_element(By.XPATH, '//td[text()=\"Country Code\"]/following-sibling::td').text\n",
        "        estc_no = driver.find_element(By.XPATH, '//td[text()=\"ESTC No\"]/following-sibling::td').text\n",
        "        format = driver.find_element(By.XPATH, '//td[text()=\"Format\"]/following-sibling::td').text\n",
        "        imprint_original = driver.find_element(By.XPATH, '//td[text()=\"Imprint Original\"]/following-sibling::td').text\n",
        "        language_code = driver.find_element(By.XPATH, '//td[text()=\"Language Code\"]/following-sibling::td').text\n",
        "        pagination = driver.find_element(By.XPATH, '//td[text()=\"Pagination\"]/following-sibling::td').text\n",
        "        title = driver.find_element(By.XPATH, '//td[text()=\"Title\"]/following-sibling::td').text\n",
        "        year = driver.find_element(By.XPATH, '//td[text()=\"Year\"]/following-sibling::td').text\n",
        "\n",
        "        return {\n",
        "            'Alt Title': alt_title,\n",
        "            'Author': author,\n",
        "            'City': city,\n",
        "            'Country Code': country_code,\n",
        "            'ESTC No': estc_no,\n",
        "            'Format': format,\n",
        "            'Imprint Original': imprint_original,\n",
        "            'Language Code': language_code,\n",
        "            'Pagination': pagination,\n",
        "            'Title': title,\n",
        "            'Year': year\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping record {record_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Function to scrape records for a given year\n",
        "def scrape_year(year):\n",
        "    records = []\n",
        "    search_url = f\"{base_url}search?q=&year={year}&year_end={year}\"\n",
        "    driver.get(search_url)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.record\"))\n",
        "            )\n",
        "            record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record a.view-record\")\n",
        "            record_urls = [element.get_attribute('href') for element in record_elements]\n",
        "\n",
        "            for record_url in record_urls:\n",
        "                record_data = scrape_record(record_url)\n",
        "                if record_data:\n",
        "                    records.append(record_data)\n",
        "\n",
        "            # Check if there is a next page\n",
        "            next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "            if 'disabled' in next_button.get_attribute('class'):\n",
        "                break\n",
        "            else:\n",
        "                next_button.click()\n",
        "                time.sleep(2)\n",
        "        except TimeoutException:\n",
        "            print(\"Timed out waiting for records to load. Moving to the next page.\")\n",
        "            break\n",
        "\n",
        "    return records\n",
        "\n",
        "### TEST 1473-1475 #####\n",
        "# Scrape records year by year\n",
        "all_records = []\n",
        "for year in range(1473, 1475):\n",
        "    print(f\"Scraping year: {year}\")\n",
        "    records = scrape_year(year)\n",
        "    all_records.extend(records)\n",
        "\n",
        "# Save to CSV and Excel\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv('ESTC_records1473_1474newfunctionscrape.csv', index=False)\n",
        "df.to_excel('ESTC_records1473_1474newfunctionscrape.xlsx', index=False)\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJhyJi8dK5oC",
        "outputId": "76a4a548-347d-4674-9985-a1d8682eaa13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping year: 1473\n",
            "Timed out waiting for records to load. Moving to the next page.\n",
            "Scraping year: 1474\n",
            "Timed out waiting for records to load. Moving to the next page.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "\n",
        "# Set up Selenium with Chrome WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# Function to scrape a single record\n",
        "def scrape_record(record_url):\n",
        "    driver.get(record_url)\n",
        "    try:\n",
        "        attributes = driver.find_elements(By.CSS_SELECTOR, 'table tbody tr')\n",
        "        record_data = {}\n",
        "        for attribute in attributes:\n",
        "            key = attribute.find_element(By.CSS_SELECTOR, 'td:nth-child(1)').text.strip()\n",
        "            value = attribute.find_element(By.CSS_SELECTOR, 'td:nth-child(2)').text.strip()\n",
        "            record_data[key] = value\n",
        "        return record_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping record {record_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape records for a given year\n",
        "def scrape_year(year):\n",
        "    records = []\n",
        "    search_url = f\"{base_url}search?q=&year={year}&year_end={year}\"\n",
        "    driver.get(search_url)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            WebDriverWait(driver, 30).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.record\"))\n",
        "            )\n",
        "            record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record a.view-record\")\n",
        "            record_urls = [element.get_attribute('href') for element in record_elements]\n",
        "\n",
        "            for record_url in record_urls:\n",
        "                record_data = scrape_record(record_url)\n",
        "                if record_data:\n",
        "                    records.append(record_data)\n",
        "\n",
        "            # Check if there is a next page\n",
        "            next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "            if 'disabled' in next_button.get_attribute('class'):\n",
        "                break\n",
        "            else:\n",
        "                next_button.click()\n",
        "                time.sleep(2)\n",
        "        except TimeoutException:\n",
        "            print(\"Timed out waiting for records to load. Moving to the next page.\")\n",
        "            break\n",
        "\n",
        "    return records\n",
        "\n",
        "# Scrape records year by year\n",
        "all_records = []\n",
        "for year in range(1473, 1475):\n",
        "    print(f\"Scraping year: {year}\")\n",
        "    records = scrape_year(year)\n",
        "    all_records.extend(records)\n",
        "\n",
        "# Save to CSV and Excel\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv('ESTC_records1473_1474_newscrape.csv', index=False)\n",
        "df.to_excel('ESTC_records1473_1474_newscrape.xlsx', index=False)\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UTu6ehkxpdi",
        "outputId": "a0820b62-d77b-40cc-d307-ca26c8655819"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping year: 1473\n",
            "Timed out waiting for records to load. Moving to the next page.\n",
            "Scraping year: 1474\n",
            "Timed out waiting for records to load. Moving to the next page.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium.common.exceptions import TimeoutException\n",
        "\n",
        "# Set up Selenium with Chrome WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# Function to scrape a single record\n",
        "def scrape_record(record_url):\n",
        "    driver.get(record_url)\n",
        "    try:\n",
        "        alt_title = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//td[text()=\"Alt Title\"]/following-sibling::td'))).text\n",
        "        author = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//td[text()=\"Author\"]/following-sibling::td'))).text\n",
        "        city = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//td[text()=\"City\"]/following-sibling::td'))).text\n",
        "        country_code = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//td[text()=\"Country Code\"]/following-sibling::td'))).text\n",
        "        estc_no = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//td[text()=\"ESTC No\"]/following-sibling::td'))).text\n",
        "        format = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//td[text()=\"Format\"]/following-sibling::td'))).text\n",
        "        imprint_original = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//td[text()=\"Imprint Original\"]/following-sibling::td'))).text\n",
        "        language_code = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//td[text()=\"Language Code\"]/following-sibling::td'))).text\n",
        "        pagination = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//td[text()=\"Pagination\"]/following-sibling::td'))).text\n",
        "        title = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//td[text()=\"Title\"]/following-sibling::td'))).text\n",
        "        year = WebDriverWait(driver, 30).until(EC.visibility_of_element_located((By.XPATH, '//td[text()=\"Year\"]/following-sibling::td'))).text\n",
        "\n",
        "        return {\n",
        "            'Alt Title': alt_title,\n",
        "            'Author': author,\n",
        "            'City': city,\n",
        "            'Country Code': country_code,\n",
        "            'ESTC No': estc_no,\n",
        "            'Format': format,\n",
        "            'Imprint Original': imprint_original,\n",
        "            'Language Code': language_code,\n",
        "            'Pagination': pagination,\n",
        "            'Title': title,\n",
        "            'Year': year\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping record {record_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape records for a given year\n",
        "def scrape_year(year):\n",
        "    records = []\n",
        "    search_url = f\"{base_url}search?q=&year={year}&year_end={year}\"\n",
        "    driver.get(search_url)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            WebDriverWait(driver, 10).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.record\"))\n",
        "            )\n",
        "            record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record a.view-record\")\n",
        "            record_urls = [element.get_attribute('href') for element in record_elements]\n",
        "\n",
        "            for record_url in record_urls:\n",
        "                record_data = scrape_record(record_url)\n",
        "                if record_data:\n",
        "                    records.append(record_data)\n",
        "\n",
        "            # Check if there is a next page\n",
        "            next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "            if 'disabled' in next_button.get_attribute('class'):\n",
        "                break\n",
        "            else:\n",
        "                next_button.click()\n",
        "                time.sleep(2)\n",
        "        except TimeoutException:\n",
        "            print(\"Timed out waiting for records to load. Moving to the next page.\")\n",
        "            break\n",
        "\n",
        "    return records\n",
        "\n",
        "# Scrape records year by year\n",
        "all_records = []\n",
        "for year in range(1473, 1475):\n",
        "    print(f\"Scraping year: {year}\")\n",
        "    records = scrape_year(year)\n",
        "    all_records.extend(records)\n",
        "\n",
        "# Save to CSV and Excel\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv('ESTC_records1473_1474.csv', index=False)\n",
        "df.to_excel('ESTC_records1473_1474.xlsx', index=False)\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGRpzR3j2Ky-",
        "outputId": "8a4181d2-c1ac-4beb-f84a-e8d196db52cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping year: 1473\n",
            "Timed out waiting for records to load. Moving to the next page.\n",
            "Scraping year: 1474\n",
            "Timed out waiting for records to load. Moving to the next page.\n"
          ]
        }
      ]
    }
  ]
}