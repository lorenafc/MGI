{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNuhQY8+nZIu9d4gOvHRpim",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lorenafc/MGI/blob/main/ESTCScrape_chromedrive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UvDuEYONlBgx"
      },
      "outputs": [],
      "source": [
        "#source: chatGPT and stack overflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium --quiet\n",
        "!pip install pandas --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CdBPs97ilI-Q",
        "outputId": "e243274d-5684-49bb-f370-1c563e42fadb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.7/467.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade selenium chromedriver-autoinstaller --quiet"
      ],
      "metadata": {
        "id": "4VgP2jRI6DEs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bEOvvNIeArqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###### ATTTEMPT 1 (FAIL) (year search included in the URL)\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "# Set up Selenium with Chrome WebDriver\n",
        "# #used StackOverflow to help adjust https://stackoverflow.com/questions/76461596/unable-to-use-selenium-webdriver-getting-two-exceptions\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")\n",
        "chrome_options.add_argument(\"--no-sandbox\")\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# Function to scrape a single record\n",
        "# got the items info from the HTML code on inspect element\n",
        "def scrape_record(record_url):\n",
        "    driver.get(record_url)\n",
        "    try:\n",
        "        WebDriverWait(driver, 30).until(\n",
        "            EC.presence_of_element_located((By.XPATH, '//td[text()=\"Title\"]/following-sibling::td'))\n",
        "        )\n",
        "        alt_title = driver.find_element(By.XPATH, '//td[text()=\"Alt Title\"]/following-sibling::td').text\n",
        "        author = driver.find_element(By.XPATH, '//td[text()=\"Author\"]/following-sibling::td').text\n",
        "        city = driver.find_element(By.XPATH, '//td[text()=\"City\"]/following-sibling::td').text\n",
        "        country_code = driver.find_element(By.XPATH, '//td[text()=\"Country Code\"]/following-sibling::td').text\n",
        "        estc_no = driver.find_element(By.XPATH, '//td[text()=\"ESTC No\"]/following-sibling::td').text\n",
        "        format = driver.find_element(By.XPATH, '//td[text()=\"Format\"]/following-sibling::td').text\n",
        "        imprint_original = driver.find_element(By.XPATH, '//td[text()=\"Imprint Original\"]/following-sibling::td').text\n",
        "        language_code = driver.find_element(By.XPATH, '//td[text()=\"Language Code\"]/following-sibling::td').text\n",
        "        pagination = driver.find_element(By.XPATH, '//td[text()=\"Pagination\"]/following-sibling::td').text\n",
        "        title = driver.find_element(By.XPATH, '//td[text()=\"Title\"]/following-sibling::td').text\n",
        "        year = driver.find_element(By.XPATH, '//td[text()=\"Year\"]/following-sibling::td').text\n",
        "\n",
        "        return {\n",
        "            'Alt Title': alt_title,\n",
        "            'Author': author,\n",
        "            'City': city,\n",
        "            'Country Code': country_code,\n",
        "            'ESTC No': estc_no,\n",
        "            'Format': format,\n",
        "            'Imprint Original': imprint_original,\n",
        "            'Language Code': language_code,\n",
        "            'Pagination': pagination,\n",
        "            'Title': title,\n",
        "            'Year': year\n",
        "        }\n",
        "    except NoSuchElementException as e:\n",
        "        print(f\"Element not found in record {record_url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping record {record_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape records for a given year\n",
        "def scrape_year(year):\n",
        "    records = []\n",
        "    search_url = f\"{base_url}search?q=&year={year}&year_end={year}\"\n",
        "    print(f\"Navigating to search URL: {search_url}\")\n",
        "    driver.get(search_url)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            WebDriverWait(driver, 30).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.record\"))\n",
        "            )\n",
        "            record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record a.view-record\")\n",
        "            record_urls = [element.get_attribute('href') for element in record_elements]\n",
        "\n",
        "            print(f\"Found {len(record_urls)} records on the page.\")\n",
        "\n",
        "            for record_url in record_urls:\n",
        "                print(f\"Scraping record: {record_url}\")\n",
        "                record_data = scrape_record(record_url)\n",
        "                if record_data:\n",
        "                    records.append(record_data)\n",
        "\n",
        "            # Check if there is a next page\n",
        "            try:\n",
        "                next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "                if 'disabled' in next_button.get_attribute('class'):\n",
        "                    print(\"Next button is disabled. No more pages.\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Clicking next button to go to the next page.\")\n",
        "                    next_button.click()\n",
        "                    time.sleep(10)\n",
        "            except NoSuchElementException:\n",
        "                print(\"Next button not found. Assuming no more pages.\")\n",
        "                break\n",
        "        except TimeoutException:\n",
        "            print(\"Timed out waiting for records to load. Moving to the next page.\")\n",
        "            break\n",
        "\n",
        "    return records\n",
        "\n",
        "### TEST 1473-1475 #####\n",
        "# Scrape records year by year\n",
        "all_records = []\n",
        "for year in range(1473, 1475):\n",
        "    print(f\"Scraping year: {year}\")\n",
        "    records = scrape_year(year)\n",
        "    all_records.extend(records)\n",
        "\n",
        "# Save to CSV and Excel\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv('ESTC_records1473_147.csv', index=False)\n",
        "df.to_excel('ESTC_records1473_147.xlsx', index=False)\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LgABSlnDkuX",
        "outputId": "4f5744eb-7292-4f08-fd7a-f13c3650e7bc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping year: 1473\n",
            "Navigating to search URL: https://estc.printprobability.org/search?q=&year=1473&year_end=1473\n",
            "Timed out waiting for records to load. Moving to the next page.\n",
            "Scraping year: 1474\n",
            "Navigating to search URL: https://estc.printprobability.org/search?q=&year=1474&year_end=1474\n",
            "Timed out waiting for records to load. Moving to the next page.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### ATTTEMPT 2 (FAIL) (trying to extract some records from the view record buttom - its not finding the records)\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "# Set up Selenium with Chrome WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
        "chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
        "\n",
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# Function to scrape a single record\n",
        "def scrape_record(record_element):\n",
        "    \"\"\"\n",
        "    Scrape data from a single record element.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract data from the record element\n",
        "        author = record_element.find_element(By.CSS_SELECTOR, 'div.authors').text.strip()\n",
        "        estc_no = record_element.find_element(By.CSS_SELECTOR, 'div.estcNo').text.strip()\n",
        "        year = record_element.find_element(By.CSS_SELECTOR, 'div.year').text.strip()\n",
        "\n",
        "        return {\n",
        "            'Author': author,\n",
        "            'ESTC No': estc_no,\n",
        "            'Year': year\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping record: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape records from the current page\n",
        "def scrape_current_page():\n",
        "    \"\"\"\n",
        "    Scrape records from the current page.\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    try:\n",
        "        record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record\")\n",
        "        print(f\"Found {len(record_elements)} records on the page.\")\n",
        "\n",
        "        for record_element in record_elements:\n",
        "            record_data = scrape_record(record_element)\n",
        "            if record_data:\n",
        "                records.append(record_data)\n",
        "\n",
        "        return records\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping current page: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to check if there's a next page and navigate to it\n",
        "def go_to_next_page():\n",
        "    \"\"\"\n",
        "    Check if there's a next page and navigate to it.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "        if 'disabled' not in next_button.get_attribute('class'):\n",
        "            print(\"Clicking next button to go to the next page.\")\n",
        "            next_button.click()\n",
        "            time.sleep(2)\n",
        "            return True\n",
        "        else:\n",
        "            print(\"Next button is disabled. No more pages.\")\n",
        "            return False\n",
        "    except NoSuchElementException:\n",
        "        print(\"Next button not found. Assuming no more pages.\")\n",
        "        return False\n",
        "\n",
        "### MAIN SCRIPT ###\n",
        "\n",
        "# Navigate to the base URL\n",
        "driver.get(base_url)\n",
        "\n",
        "# Scrape records from the main page and navigate to the next page if available\n",
        "all_records = []\n",
        "while True:\n",
        "    print(\"Scraping records from the current page.\")\n",
        "    records = scrape_current_page()\n",
        "    if records:\n",
        "        all_records.extend(records)\n",
        "\n",
        "    if not go_to_next_page():\n",
        "        break\n",
        "\n",
        "# Save to CSV and Excel\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv('ESTC_records_main_page.csv', index=False)\n",
        "df.to_excel('ESTC_records_main_page.xlsx', index=False)\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz-SzT4xlvLM",
        "outputId": "01c319b1-467c-4c15-bd69-48da7549b17b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping records from the current page.\n",
            "Found 0 records on the page.\n",
            "Next button not found. Assuming no more pages.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### ATTTEMPT 3 (FAIL) (trying to identify where time out occurs)\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import pandas as pd\n",
        "import time\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "# Set up Selenium with Chrome WebDriver\n",
        "chrome_options = Options()\n",
        "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
        "chrome_options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
        "chrome_options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
        "\n",
        "# Initialize the Chrome WebDriver\n",
        "driver = webdriver.Chrome(options=chrome_options)\n",
        "\n",
        "# Base URL of the website\n",
        "base_url = \"https://estc.printprobability.org/\"\n",
        "\n",
        "# Function to scrape a single record\n",
        "def scrape_record(record_url):\n",
        "    \"\"\"\n",
        "    Scrape data from a single record URL.\n",
        "    \"\"\"\n",
        "    driver.get(record_url)\n",
        "    try:\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.XPATH, '//td[text()=\"Title\"]/following-sibling::td'))\n",
        "        )\n",
        "        # Extract data from the record page\n",
        "        alt_title = driver.find_element(By.XPATH, '//td[text()=\"Alt Title\"]/following-sibling::td').text\n",
        "        author = driver.find_element(By.XPATH, '//td[text()=\"Author\"]/following-sibling::td').text\n",
        "        city = driver.find_element(By.XPATH, '//td[text()=\"City\"]/following-sibling::td').text\n",
        "        country_code = driver.find_element(By.XPATH, '//td[text()=\"Country Code\"]/following-sibling::td').text\n",
        "        estc_no = driver.find_element(By.XPATH, '//td[text()=\"ESTC No\"]/following-sibling::td').text\n",
        "        format = driver.find_element(By.XPATH, '//td[text()=\"Format\"]/following-sibling::td').text\n",
        "        imprint_original = driver.find_element(By.XPATH, '//td[text()=\"Imprint Original\"]/following-sibling::td').text\n",
        "        language_code = driver.find_element(By.XPATH, '//td[text()=\"Language Code\"]/following-sibling::td').text\n",
        "        pagination = driver.find_element(By.XPATH, '//td[text()=\"Pagination\"]/following-sibling::td').text\n",
        "        title = driver.find_element(By.XPATH, '//td[text()=\"Title\"]/following-sibling::td').text\n",
        "        year = driver.find_element(By.XPATH, '//td[text()=\"Year\"]/following-sibling::td').text\n",
        "\n",
        "        return {\n",
        "            'Alt Title': alt_title,\n",
        "            'Author': author,\n",
        "            'City': city,\n",
        "            'Country Code': country_code,\n",
        "            'ESTC No': estc_no,\n",
        "            'Format': format,\n",
        "            'Imprint Original': imprint_original,\n",
        "            'Language Code': language_code,\n",
        "            'Pagination': pagination,\n",
        "            'Title': title,\n",
        "            'Year': year\n",
        "        }\n",
        "    except NoSuchElementException as e:\n",
        "        print(f\"Element not found in record {record_url}: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping record {record_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to scrape records for a given year\n",
        "def scrape_year(year):\n",
        "    \"\"\"\n",
        "    Scrape records for a given year.\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    driver.get(base_url)\n",
        "\n",
        "    print(f\"Navigating to search URL: {base_url} Year search: {year}\")\n",
        "\n",
        "    try:\n",
        "        WebDriverWait(driver, 60).until(\n",
        "            EC.presence_of_element_located((By.CLASS_NAME, \"ais-Dropdown-button\"))\n",
        "        )\n",
        "\n",
        "        # Click the \"Year\" button to open the dropdown\n",
        "        print(\"Clicking the 'Year' button\")\n",
        "        year_button = driver.find_element(By.CLASS_NAME, \"ais-Dropdown-button\")\n",
        "        year_button.click()\n",
        "\n",
        "        WebDriverWait(driver, 60).until(\n",
        "            EC.presence_of_element_located((By.CLASS_NAME, \"ais-RangeInput-input--min\"))\n",
        "        )\n",
        "\n",
        "        # Input the start year\n",
        "        print(f\"Inputting the start year: {year}\")\n",
        "        start_year_input = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-input--min\")\n",
        "        start_year_input.clear()\n",
        "        start_year_input.send_keys(str(year))\n",
        "\n",
        "        # Input the end year\n",
        "        print(f\"Inputting the end year: {year}\")\n",
        "        end_year_input = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-input--max\")\n",
        "        end_year_input.clear()\n",
        "        end_year_input.send_keys(str(year))\n",
        "\n",
        "        # Click the \"Go\" button\n",
        "        print(\"Clicking the 'Go' button\")\n",
        "        go_button = driver.find_element(By.CLASS_NAME, \"ais-RangeInput-submit\")\n",
        "        go_button.click()\n",
        "\n",
        "        while True:\n",
        "            WebDriverWait(driver, 60).until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"div.record\"))\n",
        "            )\n",
        "            record_elements = driver.find_elements(By.CSS_SELECTOR, \"div.record a.view-record\")\n",
        "\n",
        "            if not record_elements:\n",
        "                print(f\"No record elements found on the page for year {year}.\")\n",
        "                break\n",
        "\n",
        "            record_urls = [element.get_attribute('href') for element in record_elements]\n",
        "            print(f\"Found {len(record_urls)} records on the page.\")\n",
        "\n",
        "            for record_url in record_urls:\n",
        "                print(f\"Scraping record: {record_url}\")\n",
        "                record_data = scrape_record(record_url)\n",
        "                if record_data:\n",
        "                    records.append(record_data)\n",
        "\n",
        "            # Check if there is a next page\n",
        "            try:\n",
        "                next_button = driver.find_element(By.CSS_SELECTOR, \"a.next\")\n",
        "                if 'disabled' in next_button.get_attribute('class'):\n",
        "                    print(\"Next button is disabled. No more pages.\")\n",
        "                    break\n",
        "                else:\n",
        "                    print(\"Clicking next button to go to the next page.\")\n",
        "                    next_button.click()\n",
        "                    time.sleep(2)\n",
        "            except NoSuchElementException:\n",
        "                print(\"Next button not found. Assuming no more pages.\")\n",
        "                break\n",
        "    except TimeoutException:\n",
        "        print(\"Timed out waiting for records to load. Moving to the next page.\")\n",
        "\n",
        "    return records\n",
        "\n",
        "### TEST 1473-1475 #####\n",
        "# Scrape records year by year\n",
        "all_records = []\n",
        "for year in range(1473, 1475):\n",
        "    print(f\"Scraping year: {year}\")\n",
        "    records = scrape_year(year)\n",
        "    all_records.extend(records)\n",
        "\n",
        "# Save to CSV and Excel\n",
        "df = pd.DataFrame(all_records)\n",
        "df.to_csv('ESTC_records1473_1474newfunctionscrape.csv', index=False)\n",
        "df.to_excel('ESTC_records1473_1474newfunctionscrape.xlsx', index=False)\n",
        "\n",
        "# Close the driver\n",
        "driver.quit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ainAv7uCmUmJ",
        "outputId": "ae1c54c0-d411-4632-cba5-1f9c6b23a3ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping year: 1473\n",
            "Navigating to search URL: https://estc.printprobability.org/ Year search: 1473\n",
            "Clicking the 'Year' button\n",
            "Inputting the start year: 1473\n",
            "Inputting the end year: 1473\n",
            "Clicking the 'Go' button\n",
            "Timed out waiting for records to load. Moving to the next page.\n",
            "Scraping year: 1474\n",
            "Navigating to search URL: https://estc.printprobability.org/ Year search: 1474\n",
            "Clicking the 'Year' button\n",
            "Inputting the start year: 1474\n",
            "Inputting the end year: 1474\n",
            "Clicking the 'Go' button\n",
            "Timed out waiting for records to load. Moving to the next page.\n"
          ]
        }
      ]
    }
  ]
}